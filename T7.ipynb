{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvNets\n",
    "\n",
    "In this notebook, we will implement the forward and backward propagation for CONV+POOL layers in numpy. These form the building blocks of convolutional neurals network which can be broken down in the followning tasks:\n",
    "\n",
    "convolution layer:\n",
    "- zero padding\n",
    "- convolve window\n",
    "- forward convolution\n",
    "- backward convolution\n",
    "\n",
    "pooling layer:\n",
    "- forward pooling\n",
    "- mask\n",
    "- sharing value\n",
    "- backward pooling\n",
    "\n",
    "Each forward operation is matched to a backward operation ; a cache is used to store parameters to compute gradients during backprop.\n",
    "\n",
    "> **Instructions:** Ensure your Python environment is setup with the following additional packages: \n",
    ">\n",
    "> - `t7utils.py` contains unit tests to check your code and some helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import t7utils as t7\n",
    "\n",
    "%matplotlib inline\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notation**:\n",
    "\n",
    "- $n_H$, $n_W$ and $n_C$: height, width and channel count of a given layer\n",
    "- lowerscript $i$: $i^{th}$ refers to the $i^{th}$ vector entry\n",
    "- superscript $[k]$ refers to an object of the $k^{th}$ layer \n",
    "- superscript $(i)$ refers object from the $i^{th}$ example. \n",
    "\n",
    "\n",
    "## A. Convolutional Layer Foward Propagation ##\n",
    "\n",
    "### 1. Zero Padding ###\n",
    "\n",
    "A convolution layer (CONV) transforms an input tensor into an output tensor of different size. For 2D data such as images, tensor dimensions are generally $n_H \\times n_W \\times n_C$. A first step consists in zero padding the input volume. This permits to use a CONV layer without reducing the width and height of the output volume. This is important when considering deeper networks as each CONV will shrink width/height. A special case is the \"same\" convolution where width/height are preserved after convolution (padding is set to 1/2 of the kernel size reduced by 1). Padding helps preserving information at the border of an image. Without padding, fewer values in the next activation maps would be affected by pixels at the edges.\n",
    "\n",
    "<img src=\"figs/zero-padding.png\" style=\"width: 800px;\"/>\n",
    "<caption><center>**Figure 1:** Zero-padding </center></caption>\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 1:**</font> Complete the following function to zero-pad the batch of examples X with zeros as illustrated in Figure 1. Hint: Use [np.pad](https://docs.scipy.org/doc/numpy/reference/generated/numpy.pad.html) to pad input tensor e.g. if you want to zero-pad `X` of shape $(2000, 32, 32, 3)$,  \n",
    "> ```python\n",
    "Y = np.pad(X, ((0,0), (2,2), (2,2), (0,0)), 'constant')\n",
    "```\n",
    "> will result in tensor or shape $(2000, 34, 34, 3)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-padding\n",
    "def zero_pad(X, pad):\n",
    "    \"\"\"\n",
    "    Zero-pad input tensor X, where padding is applied to height \n",
    "    and width as shown in Figure 1\n",
    "    \n",
    "    Argument:\n",
    "    X -- a numpy array of shape (n, n_H, n_W, n_C)\n",
    "    pad -- integer, amount of padding around height and width\n",
    "    \n",
    "    Returns:\n",
    "    X_pad -- padded numpy array of shape (n, n_H + 2 * pad, n_W + 2 * pad, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### INPUT YOUR CODE HERE ### (1 line)\n",
    "    X_pad = np.pad(X, ((0, 0), (pad, pad), (pad, pad), (0, 0)), 'constant')\n",
    "    ### END OF YOUR CODE SEGMENT ### \n",
    "    \n",
    "    return X_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape = (32, 8, 8, 3)\n",
      "X_pad.shape = (32, 12, 12, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# testing\n",
    "X = t7.zero_pad_test()\n",
    "print(\"X.shape = {}\".format(X.shape))\n",
    "X_pad = zero_pad(X, 2)\n",
    "print(\"X_pad.shape = {}\".format(X_pad.shape))\n",
    "\n",
    "_, figs = plt.subplots(1, 2)\n",
    "figs[0].set_title('X')\n",
    "figs[1].set_title('X_pad')\n",
    "figs[0].imshow(X[0,:,:,:])\n",
    "figs[1].imshow(X_pad[0,:,:,:])\n",
    "figs[1].xaxis.set_major_locator(MaxNLocator(integer=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> `X.shape = (32, 8, 8, 3)\n",
    "X_pad.shape = (32, 12, 12, 3)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Convolution Step (Window Correlation) ###\n",
    "\n",
    "Let's implement a single step of convolution by applying the filter kernel to a slice of the previous activation and corresponding to a single position of the input. The convolutional output will then be obtained by applying this single step at every position of the input.\n",
    "\n",
    "\n",
    "<img src=\"figs/convolution.png\" style=\"width: 350px;\"/>\n",
    "<caption><center>**Figure 2:** Convolution operation with a kernel of $3\\times 3$ with<br/> a stride (step of sliding window) of 3 assuming zero bias</center></caption>\n",
    "\n",
    "Each value in the left matrix corresponds to a single pixel value. We convolve a $3\\times 3$ filter with the image by multiplying its values element-wise, summing up and adding a bias. \n",
    "\n",
    "> <font color='darkgreen'>**Exercise 2:**</font> Complete the following function to implement a single convolution step corresponding to applying a filter to one position and resulting in a single scalar output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single convolution step\n",
    "def conv_step(a_prev_slice, W, b):\n",
    "    \"\"\"\n",
    "    Apply conv filter (W, b) on single slice\n",
    "    \n",
    "    Arguments:\n",
    "    a_prev_slice -- input slice of shape (n_f, n_f, n_C_prev)\n",
    "    W -- weight matrix of shape (n_f, n_f, n_C_prev)\n",
    "    b -- bias scalar\n",
    "    \n",
    "    Returns:\n",
    "    z -- scalar\n",
    "    \"\"\"\n",
    "   \n",
    "    ### INPUT YOUR CODE HERE ### (1 line)\n",
    "    z = np.sum(a_prev_slice * W) + b\n",
    "    ### END OF YOUR CODE SEGMENT ### \n",
    "\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z = -7.636\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "a_prev_slice, W, b = t7.conv_step_test()\n",
    "z = conv_step(a_prev_slice, W, b)\n",
    "print(\"z = {:.3f}\".format(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> `z = -7.636`\n",
    "\n",
    "### 3. CONV Layer Forward ###\n",
    "\n",
    "In the forward propagation, we consider a number of filters and convolve them on the input. Each 'convolution' results in a 2D convolution map that are stacked to form a 3D volume.\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 3:**</font>  Implement the following function to convolve the filters $(W, b)$ on the activation map of the previous layer $A^{[k-1]}$ for a batch of $n$ samples. The function takes as input `A_prev`, $n_C$ filter weights denoted by `W` of shape $(n_f, n_f, n_C^{[k-1]}, n_C^{[k]})$, and a bias vector denoted by `b` of shape $(1, 1, 1, n_C^{[k]})$ where each filter has its own single bias. Finally we also pass an hyperparameter dictionary, `params`, containing stride and padding details.\n",
    "> \n",
    "> **Hint:** to select a $n_f \\times n_f$ slide at the $(i,j)$ location of the previous activation map, use `A_prev[i:i+n_f, j:j+n_f, :]`\n",
    "> \n",
    "> $$ n_H^{[k]} = \\left\\lfloor \\frac{n_{H}^{[k-1]} - n_f + 2 \\times padding}{stride} \\right\\rfloor +1 $$\n",
    ">\n",
    "> $$ n_W^{[k]} = \\left\\lfloor \\frac{n_{W}^{[k-1]} - n_f + 2 \\times padding}{stride} \\right\\rfloor +1 $$\n",
    ">\n",
    "> $$ n_C^{[k]} = \\text{number of convolution filters}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward convolution\n",
    "def conv_fwd(A_prev, W, b, params):\n",
    "    \"\"\"\n",
    "    Implements convolution forward propagation \n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- activations of previous layer, array of shape (n, n_H_prev, n_W_prev, n_C_prev)\n",
    "    W -- weights tensor of shape (n_f, n_f, n_C_prev, n_C)\n",
    "    b -- bias vector of shape (1, 1, 1, n_C)\n",
    "    params -- python dictionary \n",
    "        stride -- integer\n",
    "        padding -- integer \n",
    "        \n",
    "    Returns:\n",
    "    Z -- convolution output tensor of shape (n, n_H, n_W, n_C)\n",
    "    cache -- dictionary for backprop\n",
    "        A_prev -- activations of previous layer, array of shape (n, n_H_prev, n_W_prev, n_C_prev)\n",
    "        W -- weights tensor of shape (n_f, n_f, n_C_prev, n_C)\n",
    "        b -- bias vector of shape (1, 1, 1, n_C)\n",
    "        params -- python dictionary \n",
    "            stride -- integer\n",
    "            padding -- integer \n",
    "    \"\"\"\n",
    "    \n",
    "    ### INPUT YOUR CODE HERE ### (8 lines)\n",
    "    (n, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape # Retrieve dimensions\n",
    "    (n_f, n_f, n_C_prev, n_C) = W.shape # Retrieve dimensions\n",
    "    stride = params[\"stride\"] # Retrieve stride\n",
    "    padding = params[\"padding\"] # Retrieve padding\n",
    "    n_H = int((n_H_prev - n_f + 2 * padding) / stride) + 1 # Compute kernel width\n",
    "    n_W = int((n_W_prev - n_f + 2 * padding) / stride) + 1 # Compute kernel height\n",
    "    Z = np.zeros((n, n_H, n_W, n_C)) # Initialize output tensor with zeros\n",
    "    A_prev_pad = zero_pad(A_prev, padding) # Pad input activation maps\n",
    "    ### END OF YOUR CODE SEGMENT ###         \n",
    "\n",
    "    for i in range(n):                    # batch loop\n",
    "        a_prev_pad = A_prev_pad[i,:,:,:]  # select ith training padded activation  \n",
    "        for h in range(n_H):              # height loop\n",
    "            for w in range(n_W):          # width loop\n",
    "                for c in range(n_C):      # channel (# of filter) loop\n",
    "                    ### INPUT YOUR CODE HERE ### (4 lines)\n",
    "                    h_range = slice(h*stride, h*stride+n_f)\n",
    "                    w_range = slice(w*stride, w*stride+n_f)\n",
    "                    a_slice_prev = a_prev_pad[h_range, w_range, :]\n",
    "                    Z[i, h, w, c] = np.sum(a_slice_prev * W[..., c]) + b[..., c]\n",
    "                    ### END OF YOUR CODE SEGMENT ###         \n",
    "\n",
    "    assert(Z.shape == (n, n_H, n_W, n_C))\n",
    "    cache = {}\n",
    "    ### INPUT YOUR CODE HERE ### (4 lines)\n",
    "    cache['A_prev'] = A_prev\n",
    "    cache['W'] = W\n",
    "    cache['b'] = b\n",
    "    cache['params'] = params\n",
    "    ### END OF YOUR CODE SEGMENT ###          \n",
    "\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean(Z) = -0.162\n",
      "cache.keys = ['A_prev', 'W', 'b', 'params']\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "A_prev, W, b, params = t7.conv_fwd_test()\n",
    "Z, cache = conv_fwd(A_prev, W, b, params)\n",
    "print(\"mean(Z) = {:.3f}\".format(np.mean(Z)))\n",
    "print(\"cache.keys = {}\".format([key for key in cache.keys()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> `mean(Z) = -0.162\n",
    "cache.keys = ['A_prev', 'W', 'b', 'params']`\n",
    "\n",
    "The convolution activation map is finally obtained by applying a non-linearity to the (linear) convolution ouput `Z` e.g. ReLU:\n",
    "\n",
    "```python\n",
    "# Convolution\n",
    "Z, conv_cache = conv_fwd(A_prev, W, b, params)\n",
    "# ReLU non-linearity\n",
    "A, activation_cache = t7.relu_fwd(Z)\n",
    "```\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 4:**</font>  Implement the forward convolution layer combine the forward convolution and ReLU activation. Assume the existence of `t7.relu_fwd(Z)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional layer forward propagation ReLU activation\n",
    "def convlayer_fwd(A_prev, W, b, params):\n",
    "    \"\"\"\n",
    "    Conv layer forward propagation (conv + non-linearity)\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- activations of previous layer, array of shape (n, n_H_prev, n_W_prev, n_C_prev)\n",
    "    W -- weights tensor of shape (n_f, n_f, n_C_prev, n_C)\n",
    "    b -- bias vector of shape (1, 1, 1, n_C)\n",
    "    params -- python dictionary \n",
    "        stride -- integer\n",
    "        padding -- integer \n",
    "        \n",
    "    Returns:\n",
    "    A -- activation output tensor of shape (n, n_H, n_W, n_C)\n",
    "    cache -- dictionary for backprop\n",
    "        CONV -- dictionary convolution cache\n",
    "        ACTIVATION -- dictionary activation cache  \n",
    "    \"\"\"\n",
    "        \n",
    "    ### INPUT YOUR CODE HERE ### (2 lines)\n",
    "    Z, conv_cache = conv_fwd(A_prev, W, b, params)\n",
    "    A, activation_cache = t7.relu_fwd(Z)\n",
    "    ### END OF YOUR CODE SEGMENT ### \n",
    "    return A, {'CONV': conv_cache, 'ACTIVATION': activation_cache}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean(A) = 1.946\n",
      "cache.keys = [['A_prev', 'W', 'b', 'params'], ['Z']]\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "A_prev, W, b, params = t7.conv_fwd_test()\n",
    "A, cache = convlayer_fwd(A_prev, W, b, params)\n",
    "print(\"mean(A) = {:.3f}\".format(np.mean(A)))\n",
    "print(\"cache.keys = {}\".format([key for key in [[key for key in value.keys()] for _, value in cache.items()]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> `mean(A) = 1.946\n",
    "cache.keys = [['A_prev', 'W', 'b', 'params'], ['Z']]`\n",
    "\n",
    "## B. Convolutional Layer Backprop ##\n",
    "\n",
    "When using deep learning frameworks, you only implement the forward pass, and the backward is derived automatically. In the case of convolutional layers, the backward pass is a little more involved.\n",
    "\n",
    "### 1. Convolution Backprop ###\n",
    "\n",
    "The partial derivative of the loss with respect to the convolutional layer activation, $dA$, for a given filter kernel $c$ and a given training example is given by:\n",
    "\n",
    "$$ dA += \\sum _{h=1} ^{n_H} \\sum_{w=1} ^{n_W} W_c \\times dZ_{hw} \\tag{1}$$\n",
    "\n",
    "where $W_c$ is the filter kernel for the $c^{th}$ activation map and $dZ_{hw}$ is a scalar corresponding to the gradient of the loss with respect to the output of the convolution layer Z at the $h^{th}$ row and $w^{th}$ column (corresponding to the dot product taken at the i-th stride left and j-th stride down). Note that at each location $(w, h)$, the same filter $W_c$ is multiplied by a different $dZ_{hw}$ when updating $dA$. We do so because of the parameter sharing in the convolution, each filter is dotted and summed by a different activation slice. Therefore when computing the backprop for dA, we are just adding the gradients of all the slices. This translates into the following code segment:\n",
    "\n",
    "```python \n",
    "da_prev_pad[h*stride:h*stride+n_f, w*stride:w*stride+n_f, :] += W[:,:,:,c] * dZ[i, h, w, c] \n",
    "```\n",
    "\n",
    "The partial derivative of the loss with respect to the weights of the $c^{th}$ filter kernel is given by:\n",
    "\n",
    "$$ dW_c  += \\sum _{h=1} ^{n_H} \\sum_{w=1} ^ {n_W} a_{slice} \\times dZ_{hw}  \\tag{2}$$\n",
    "\n",
    "where $a_{slice}$ corresponds to the slice used to generate the activation $Z_{hw}$. Hence, this provides the gradient for $W_c$ with respect to that slice. Since it is the same $W_c$ for all slices, we just add up all these gradients to get $dW_c$. This translates into the following code segment:\n",
    "\n",
    "```python \n",
    "dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "```\n",
    "\n",
    "The partial derivative of the loss with respect to the bias of the $c^{th}$ filter kernel is given by:\n",
    "\n",
    "$$ db_c += \\sum _{h=1} ^{n_H} \\sum_{w=1} ^ {n_W} dZ_{hw} \\tag{3}$$\n",
    "\n",
    "This translates into the following code segment:\n",
    "\n",
    "```python\n",
    "db[:,:,:,c] += dZ[i, h, w, c]```\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 5:**</font> Implement the `conv_backward` function below. You should sum over all the training examples, filters, heights, and widths. You should then compute the derivatives using formulas 1, 2 and 3 above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward convolution\n",
    "def conv_back(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implements convolution backward propagation \n",
    "    \n",
    "    Arguments:\n",
    "    dZ -- gradient of loss with respect to conv layer output, array of shape (n, n_H, n_W, n_C)\n",
    "    cache -- dictionary for backprop\n",
    "        A_prev -- activations of previous layer, array of shape (n, n_H_prev, n_W_prev, n_C_prev)\n",
    "        W -- weights tensor of shape (n_f, n_f, n_C_prev, n_C)\n",
    "        b -- bias vector of shape (1, 1, 1, n_C)\n",
    "        params -- python dictionary \n",
    "            stride -- integer\n",
    "            padding -- integer \n",
    "    \n",
    "    Returns:\n",
    "        dW -- gradient of loss with respect to current layer weights, shape (n_f, n_f, n_C_prev, n_C)\n",
    "        db -- gradient of loss with respect to current layer bias, shape (1, 1, 1, n_C)\n",
    "        dA_prev -- gradient of loss with respect to activation of previous layer output, shape (n, n_H_prev, n_W_prev, n_C_prev)\n",
    "    \"\"\"    \n",
    "    \n",
    "    ### INPUT YOUR CODE HERE ### (14 lines)\n",
    "    A_prev = cache[\"A_prev\"] # Retrieve from forward prop cache\n",
    "    W = cache[\"W\"] # Retrieve from forward prop cache\n",
    "    b = cache[\"b\"] # Retrieve from forward prop cache\n",
    "    params = cache[\"params\"] # Retrieve from forward prop cache\n",
    "    (n, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape # Retrieve dimensions\n",
    "    (n_f, n_f, n_C_prev, n_C) = W.shape # Retrieve dimensions\n",
    "    stride = params[\"stride\"] # Retrieve stride\n",
    "    padding = params[\"padding\"] # Retrieve padding\n",
    "    (n, n_H, n_W, n_C) = dZ.shape # Retrieve dimensions\n",
    "    A_prev_pad = zero_pad(A_prev, padding) # Pad A_prev\n",
    "    dA_prev_pad = np.zeros_like(A_prev_pad) # Initialise dA_prev_pad\n",
    "    dW = np.zeros_like(W) # Initialise dW\n",
    "    db = np.zeros_like(b) # Initialise db\n",
    "    dA_prev = np.zeros_like(A_prev) # Initialise dA_prev\n",
    "    ### END OF YOUR CODE SEGMENT ###          \n",
    "\n",
    "    \n",
    "    for i in range(n):                    # batch loop\n",
    "        a_prev_pad = A_prev_pad[i,:,:,:]  # select ith training padded activation  \n",
    "        da_prev_pad = dA_prev_pad[i,:,:,:]# select ith training padded activation gradient  \n",
    "        for h in range(n_H):              # height loop\n",
    "            for w in range(n_W):          # width loop\n",
    "                for c in range(n_C):      # channel (# of filter) loop\n",
    "                    ### INPUT YOUR CODE HERE ### (6 lines)\n",
    "                    h_range = slice(h*stride, h*stride+n_f)\n",
    "                    w_range = slice(w*stride, w*stride+n_f)\n",
    "                    a_slice = a_prev_pad[h_range, w_range, :] # slice of a_prev_pad\n",
    "                    da_prev_pad[h_range, w_range, :] += W[:, :, :, c] * dZ[i, h, w, c]\n",
    "                    dW[:, :, :, c] += a_slice * dZ[i, h, w, c]\n",
    "                    db[:, :, :, c] += dZ[i, h, w, c]\n",
    "                    ### END OF YOUR CODE SEGMENT ###\n",
    "        \n",
    "        ### INPUT YOUR CODE HERE ### (1 line)\n",
    "        dA_prev[i, :, :, :] = da_prev_pad[padding:-padding, padding:-padding, :] # Remove padding\n",
    "        ### END OF YOUR CODE SEGMENT ###\n",
    "    \n",
    "    assert(dA_prev.shape == (n, n_H_prev, n_W_prev, n_C_prev))\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean(dA) = 0.961\n",
      "mean(dW) = 1162.824\n",
      "mean(db) = -6010.032\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "A_prev, W, b, params = t7.conv_fwd_test()\n",
    "Z, cache_conv = conv_fwd(A_prev, W, b, params)\n",
    "dA, dW, db = conv_back(Z, cache_conv)\n",
    "print(\"mean(dA) = {:.3f}\".format(np.mean(dA)))\n",
    "print(\"mean(dW) = {:.3f}\".format(np.mean(dW)))\n",
    "print(\"mean(db) = {:.3f}\".format(np.mean(db)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> `mean(dA) = 0.961\n",
    "mean(dW) = 1162.824\n",
    "mean(db) = -6010.032`\n",
    "\n",
    "### 2. CONV Layer Backprop ###\n",
    "\n",
    "We can now combine the backprop from the convolution and non-lineary modules to compute the backprop for a single convolutional layer.\n",
    "\n",
    "<font color='darkgreen'>**Exercise 6:**</font> Implement the convolution layer backward propagation. Assume the existence of `t7.relu_back(dA, cache)`<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional layer backward propagation with ReLU activation\n",
    "def convlayer_back(dA, cache):\n",
    "    \"\"\"\n",
    "    Single convolutional layer backprop\n",
    "\n",
    "    Arguments:\n",
    "    dA -- gradient of loss with respect to activation\n",
    "    cache -- dictionary from forward propagation\n",
    "        CONV -- dictionary convolution cache\n",
    "        ACTIVATION -- dictionary activation cache \n",
    "\n",
    "    Returns:\n",
    "    dW -- gradient of loss with respect to current layer weights\n",
    "    db -- gradient of loss with respect to current layer bias\n",
    "    dA_prev -- gradient of loss with respect to activation of previous layer output\n",
    "    \"\"\"\n",
    "    \n",
    "    conv_cache = cache['CONV']\n",
    "    activation_cache = cache['ACTIVATION']\n",
    "    \n",
    "    ### INPUT YOUR CODE HERE ### (2 lines)\n",
    "    dZ = t7.relu_back(dA, activation_cache)\n",
    "    dW, db, dA_prev = conv_back(dZ, conv_cache)\n",
    "    ### END OF YOUR CODE SEGMENT ### \n",
    "    \n",
    "    return dW, db, dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean(dA_prev) = -50.620\n",
      "mean(dW) = -0.010\n",
      "mean(db) = 6.885\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "dA, cache = t7.convlayer_back_test()\n",
    "dW, db, dA_prev = convlayer_back(dA, cache)\n",
    "print(\"mean(dA_prev) = {:.3f}\".format(np.mean(dA_prev)))\n",
    "print(\"mean(dW) = {:.3f}\".format(np.mean(dW)))\n",
    "print(\"mean(db) = {:.3f}\".format(np.mean(db)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> `mean(dA_prev) = -50.620\n",
    "mean(dW) = -0.010\n",
    "mean(db) = 6.885`\n",
    "\n",
    "## C. Pooling Layer Forward Propagation ##\n",
    "\n",
    "The pooling (POOL) layer reduces the height and width of the activation input maps. Not only this helps in reducing computation but it also contributes to learning spatial invariance features. There are two main types of pooling layers as shown Figure 3, namely: \n",
    "\n",
    "- max pool: slides an $n_f \\times n_f$ window over the input and extracts the maximum value\n",
    "\n",
    "- average pool: slides an $n_f \\times n_f$ window over the input and compute the average value\n",
    "\n",
    "<img src=\"figs/pooling.png\" style=\"width: 710px;\"/>\n",
    "<caption><center>**Figure 3:** Pooling operation with a window of $3\\times 3$ and a stride of 3</center></caption>\n",
    "\n",
    "Note that POOL layers have no trainable parameters for backprop. However, they define hyperparameters such as the window size $n_f$. Let's now implement both MAXPOOL and AVGPOOL as a single function API. The dimensions of the pooling output are given by:\n",
    "\n",
    "$$ n_H^{[k]} = \\left\\lfloor \\frac{n_H^{[k-1]} - n_f}{stride} \\right\\rfloor + 1 $$\n",
    "\n",
    "$$ n_W^{[k]} = \\left\\lfloor \\frac{n_W^{[k-1]} - n_f}{stride} \\right\\rfloor + 1 $$\n",
    "\n",
    "$$ n_C^{[k]} = n_C^{[k-1]}$$\n",
    "\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 7:**</font> Implement the forward pass for the POOL layer<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pooling layer forward propagation\n",
    "def pool_fwd(A_prev, params):\n",
    "    '''\n",
    "    Implements pooling forward propagation \n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- activations of previous layer, tensor of shape (n, n_H_prev, n_W_prev, n_C_prev)\n",
    "    params -- python dictionary \n",
    "        stride -- integer\n",
    "        n_f -- integer\n",
    "        pool_mode -- string either 'max' or 'avg'\n",
    "    \n",
    "    Returns:\n",
    "    A -- pool layer output, tensor of shape (n, n_H_prev, n_W_prev, n_C_prev)\n",
    "    cache -- dictionary for backprop\n",
    "        A_prev -- pool layer input\n",
    "        stride -- integer\n",
    "        n_f -- integer\n",
    "        pool_mode -- string either 'max' or 'avg'\n",
    "    '''\n",
    "    \n",
    "    ### INPUT YOUR CODE HERE ### (8 lines)\n",
    "    (n, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape # Retrieve dimensions\n",
    "    stride = params['stride'] # Retrieve stride\n",
    "    n_f = params['n_f']    # Retrieve n_f\n",
    "    pool_mode = params['pool_mode'] # Retrieve pool_mode\n",
    "    n_H = int(1 + (n_H_prev - n_f) / stride)\n",
    "    n_W = int(1 + (n_W_prev - n_f) / stride)\n",
    "    n_C = n_C_prev\n",
    "    A = np.zeros((n, n_H, n_W, n_C)) # Initialize output tensor with zeros         \n",
    "    ### END OF YOUR CODE SEGMENT ###\n",
    "    \n",
    "    for i in range(n):                    # batch loop\n",
    "        for h in range(n_H):              # height loop\n",
    "            for w in range(n_W):          # width loop\n",
    "                for c in range(n_C):      # channel loop\n",
    "                    ### INPUT YOUR CODE HERE ### (7 lines)\n",
    "                    h_range = slice(h*stride, h*stride+n_f)\n",
    "                    w_range = slice(w*stride, w*stride+n_f)\n",
    "                    a_prev_slice = A_prev[i, h_range, w_range, c]\n",
    "                    if pool_mode == 'max':\n",
    "                        A[i, h, w, c] = np.max(a_prev_slice)\n",
    "                    elif pool_mode == 'avg': \n",
    "                        A[i, h, w, c] = np.mean(a_prev_slice)\n",
    "                    ### END OF YOUR CODE SEGMENT ###\n",
    "        \n",
    "    cache = {}\n",
    "    cache['A_prev'] = A_prev\n",
    "    cache['stride'] = stride\n",
    "    cache['n_f'] = n_f\n",
    "    cache['pool_mode'] = pool_mode\n",
    "    \n",
    "    assert(A.shape == (n, n_H, n_W, n_C))\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean(A_maxpool) = 0.900\n",
      "mean(A_avgpool) = 0.501\n",
      "3x3 CONV with stride 3\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAC6CAYAAAAzgU7DAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAj/0lEQVR4nO3deVhV1f4/8PdhOoATKMogiIJm4E9RQcPSQG9GBiYazoqmdkvt5lSaZmqDpZnXITVNS1RwiDQ15wFMvZKhfLGcbmLORCSi5sD8+f3Rc7gdDywOw3Fov1/P4/PIfq+99jqcvTefs9l7oRMRAREREWmW1YMeABERET1YLAaIiIg0jsUAERGRxrEYICIi0jgWA0RERBrHYoCIiEjjWAwQERFpHIsBIiIijWMxQEREpHH3tRiIiYmBTqfD+fPn7+dmS3Xnzh1MmzYN+/btK9d6Bw4cgF6vx4ULF4qXHTx4EMOGDUNgYCD0er3ydWZkZOC1116Dj48PHBwc4O3tjaFDh+LixYsVfi1btmxBdHQ0mjdvDltbW+h0ulLbpqWlYeDAgWjQoAEcHBzg6+uLsWPHIisry6jdwIEDERkZWeExUdUIDQ2FTqcr/ufg4ICAgADMnTsXRUVFRm1FBKtXr0anTp3g7OwMvV4PHx8fjBw5EpcuXSqx//KuM3jwYFSvXr3Cr6ek4wcA8vPz8e9//xvNmzeHg4MDnJyc8OSTT+LQoUMV3hYAbNq0CSEhIahZsyaqVauGZs2a4fPPPzfarq+vL+bOnVup7RAwbdo0o33Vzs4OjRo1wqhRo3D9+nWT9t9//z169uwJd3d32NnZwc3NDVFRUUhKSip1G+VZx/Az58iRIxV6PdevX4eLiwvWrl1rkpW1X1XE8ePH0bNnT9StWxd6vR4NGzbEiBEjjNpY7Lws91FmZqYkJSVJTk7O/dxsqX7//XcBIFOnTjV7naKiImndurWMHDnSaPm0adPE29tbIiMjJTQ0VADIuXPnTNbPycmRJk2aiIuLiyxcuFASExNl8eLF4urqKvXr15ebN29W6LUMGTJEmjRpIr169ZLAwEAp7a3NzMyUOnXqSKNGjSQmJkYSEhJk9uzZUr16dWnZsqUUFhYWt01LSxMbGxvZu3dvhcZEVSMkJER8fHwkKSlJkpKSZNOmTdKlSxcBIOPHjy9uV1hYKL179xYA0rdvX9m4caMkJibKvHnzxNPTU5ycnOTgwYNGfVdknUGDBkm1atUq9FpKO34KCgokPDxcatWqJdOnT5fExETZsmWLvPvuu7Jr164KbUtE5KOPPhIrKysZMWKEbN++Xfbs2SMLFiyQTz/91KhdTEyMODs7y9WrVyu8LRKZOnWqAJAdO3ZIUlKS7Nq1S0aPHi06nU6Cg4OlqKiouO38+fPFyspKgoODZeXKlfLdd9/JqlWrJDg4WKysrEzeo4qss3z5cgEgycnJFXo9o0ePlubNmxuNW8T8/ao8EhISxMHBQZ599ln5+uuvZd++fbJy5UoZM2aMUTtLnZfvazHwsKlIMbBt2zYBIKdPnzZa/tcforNmzSq1GNi9e7cAkGXLlhktX716tQCQDRs2lOs1lLT9kSNHlloMLF26VADInj17jJZ/+OGHAkBSUlKMlkdEREjnzp0rNCaqGiEhIdKsWTOjZXl5eeLj4yOOjo6Sl5cnIv97D2fMmGHSR0ZGhnh7e4urq6tkZ2cXL6/IOpUpBko7fubMmSNWVlaSlJRUoX5LcuTIEbGyspKZM2eW2TY3N1dq164t06dPr7Lta5GhGPj999+Nlg8cOFAAFBeWBw8eFCsrK4mIiJD8/Hyjtvn5+RIRESFWVlZGhWhF1qlMMZCVlSUODg6yePFio+Xl2a/Mdfv2bXF3d5fw8HCTwqMkljgv39diwPDG/PWHpOFE98MPP0j79u3FwcFBGjVqJB999JHRD7jExEQBIKtWrZIxY8aIq6ur2Nvby9NPP23yAywkJERCQkJMtj9o0CDx9vYWEZFz584JAJN/gwYNUr6Grl27Sps2bZRtVMXAvn37BIDEx8cbLTecJLdt26bs2xyqYiAmJqbEg2PRokUCQE6ePGm0fN26daLT6SQtLa3S43rYGE5cx44dk6ioKKlZs6Y4OzvLmDFjJD8/X06fPi1hYWFSvXp18fb2Njn47969K2PHjpWAgIDidYODg2Xjxo1G7dasWSMATD41TJkyRaysrMr85FtSMSAi0rNnTwEgV65ckdzcXHF2dhY/P79STyaGgvOTTz4REanQOiKVKwZKO34aNmwooaGhFeqzNIMHDxYHBwe5c+eOWe2HDx8u3t7eZp2MHxZnzpyRwYMHS+PGjcXBwUE8PDwkIiJCfvzxx+I2mZmZYmtrK5MnTzZZ/9SpUwJA5s2bV7zswIEDEhwcLHq9Xjw8PGTy5MnFHyJKOqf9VWnFwMKFCwWAxMXFiYhIeHi4WFtby6VLl0rs5+LFi2JtbS0RERHFyyqyTmWKgdmzZ4utra1RISxS/v3KHIbz8r59+8xqb4nz8kNxA2FGRgb69++PAQMGYPPmzejSpQsmTpyI2NhYk7aTJk3CL7/8gmXLlmHZsmVIT09HaGgofvnll3Jt093dHTt27AAADB06FElJSUhKSsI777xT6jp5eXnYs2cPOnbsWL4X+BdPPfUUAgMDMW3aNCQnJ+PWrVtISUnBpEmT0Lp1azzzzDMV7tsckZGRaNCgAcaNG4cTJ07g1q1b2L9/P2bMmIGuXbvCz8/PqH1oaChEBNu2bbPouB6kXr16ISAgAOvXr8fLL7+MOXPmYMyYMYiMjER4eDi++eYbdOrUCRMmTMCGDRuK18vNzcW1a9fwxhtvYOPGjVizZg3at2+PHj16YOXKlcXt+vTpg1dffRXjxo0r/t1lQkICPvjgA0yaNAmdO3eu0LjPnj0LGxsbODs74+jRo8jOzsYLL7xQ6v0iXbt2hZWVFXbv3g0AFVqnMko7fi5duoTz58+jefPmmDRpElxdXWFjY4NmzZphxYoVFd7e/v374efnh/Xr16Np06awtraGp6cn3nrrLeTl5Zm0Dw0NxYULF3D8+PEKb/N+S09PR506dTBjxgzs2LEDCxcuhI2NDZ544gn897//BQDUrVsXERERWLFihck9JsuXL4ednR369+8PAPjxxx/RuXNn3LlzBytWrMDixYuRkpKC6dOnV2qcaWlpxWMpLCxEYmIigoKC4OnpWWJ7Ly8vBAYGIiEhAYWFhRVap7K2bt2KVq1awcnJyWh5efcrc+zfvx8AUFhYiPbt28POzg7Ozs7o27cv0tPTTdpb5LxcZWWFGUq7MgBADh8+bNTW399fwsLCir82XBlo3bq1UeV+/vx5sbW1lWHDhhn1WdaVAZHy/5rg8OHDAkDWrl2rbKe6MiAicvPmTenatavRFYnQ0FDJysoyaxxlUV0ZEBFJT0+Xdu3aGW2/Z8+epd7LUb9+fendu3eVjO1hYvgUM3v2bKPlLVu2NPmVTX5+vtStW1d69OhRan8FBQWSn58vQ4cOlVatWhllOTk50qpVK2nUqJGcPHlSXF1dJSQkRAoKCsocp+HKQH5+vuTn50t6erq89dZbxe+biMjatWsFgMklzXu5urqKn59fhdcRqfiVgdKOn6SkJAEgNWvWFH9/f/nqq69k586dEhUVJQDk888/L/e2RET0er3UqFFDnJ2dZcGCBZKQkCBvv/22WFtbS79+/UzanzlzRgDIZ599VqHtPQwKCgokLy9PmjRpYvS75s2bNwsAo6tQBQUF4uHhIS+++GLxsp49e0q1atWMPtkXFhaKv79/ua4MZGRkSH5+vmRnZ0tsbKw4ODiIl5eX3L17VzIyMgSA9OnTR9mX4V6W3377rULriFTuyoCjo6O8+uqrJsvLu1+ZIywsTACIk5OTjB8/XhISEmTx4sVSp04dady4sdy+fdtknao+L9tUXVlRcW5ubmjbtq3RshYtWiA1NdWkbb9+/Yw+xXh7e+PJJ59EYmKipYdZXKHVq1evwn3k5+ejd+/eOH78OJYuXYqmTZvi3Llz+OCDD9C5c2ckJCSgVq1aVTVkE9nZ2ejWrRvu3LmDuLg4eHl54fjx43j//ffxwgsvYOvWrbCxMd4t6tWrhytXrlhsTA9aRESE0dd+fn44duwYunTpUrzMxsYGjRs3NrkDPj4+HnPnzsWxY8dw+/bt4uX29vZG7fR6Pb766isEBgaidevWqFmzJtasWQNra2uzxnjixAnY2toWf21ra4v+/ftj4cKFZr9O4M8nB1RPmlTVOiUp7fgxfFrNycnBtm3b4O3tDQDo3LkzgoKC8N577+Hll18u9/aKiorwxx9/YM2aNejTpw8AoGPHjrh9+zbmzp2Ld999F40bNy5ubxjXo7SvFxQU4OOPP0ZsbCzS0tKQn59fnJ06dar4/126dIGbmxuWL19efCVq586dSE9Px5AhQ4rbfffdd+jUqRNcXFyKl1lZWaFXr16YNm2a2eNyc3Mz+vqpp57C559/Dnt7e9y4ccOsPkQEAKDT6Yr/X551KuP69eu4c+dOief68u5X5jAcA71798bMmTOL+3Rzc0NkZCRWr16NYcOGGa1T1eflh+LXBHXq1DFZptfrcffuXZPl9+5khmX3PhZnCYbx3HuiL48vvvgC27dvx4YNGzBs2DB06NAB0dHR2LFjB1JSUiz+eNPMmTORmpqK3bt3o1+/fujQoQOGDx+OuLg47Nq1C3FxcSbr2Nvbl/he/F3Url3b6Gs7Ozs4OjqavM92dnbIyckp/nrDhg3o1asX6tevj9jYWCQlJSE5ORlDhgwxamfQuHFjdOjQATk5Oejfvz/c3d3NHqOvry+Sk5Nx5MgRHD9+HNevX0dsbGxx4digQQMAwLlz50rt4/bt27h69Sq8vLwqvE5llHb8GI7/xx9/vLgQAP48oYeFheHy5cvIzMws9/YM/YaFhRktNxR5KSkpRssN43qU9vWxY8finXfeQWRkJL799lscPnwYycnJCAgIMHodNjY2GDhwIL755pviR/xiYmLg7u5u9P3JysqCq6uryXZKWqayZ88eJCcnIzU1FVevXsXBgwfh7+8PAHBxcYGjo6NyvwOA8+fPw9HREbVr167QOpWhOteXd78yR2l9hoWFQafTldhnVZ+XH4pioDwyMjJKXPbXgsLe3h65ubkm7a5evVqpbRuq5WvXrlW4j9TUVFhbW6N169ZGy318fFCnTh2L/74yNTUV9evXN/lB1KZNGwAocfvXrl0z+qRAf4qNjUWjRo2wbt06REZGIjg4GEFBQSXuewCwbNkybN26FW3btsWCBQtw+PBhs7dlb2+PoKAgBAYGolmzZnB0dDTKAwMD4ezsjM2bN5f6KWrz5s0oKioq/mRYkXUqo7Tjx9fX1+T1GBjGZWVV/lNVixYtytWnYVyP0r4eGxuL6OhofPjhhwgLC0Pbtm0RFBRU4rnupZdeQk5ODtauXYvs7Gxs3rwZ0dHRRlen6tSpg99++81k3ZLOuyoBAQEICgpCQECAyYc9a2trdOzYEUeOHMHly5dLXP/y5cs4evQoOnXqBGtr6wqtUxmGMZd0ri/vfmWO0vo0KKnPqj4vP3LFwJo1a4xOXBcuXMChQ4cQGhpavKxhw4b4+eefjU7KWVlZJpOX6PV6AOZ/EjDcXHf27NmKDh8eHh4oLCxEcnKy0fKff/4ZWVlZpd4cU1U8PDxw+fJlk8tLhgk77t1+QUEBLl26VFzV0/8YJlX56yXJjIwMbNq0yaTtTz/9hNdffx3R0dE4cOAAWrRogd69eyM7O7tKxmJnZ4c333wTp06dwqxZs0zyzMxMTJw4Ea6ursWXGyuyTmWUdvzY2NigW7duOHXqlNFEXSKCHTt2wNfXt0InvRdffBEAsH37dqPl27Ztg5WVVXEBbGC4CflR2td1Ol3xecxg69atJV4+9vPzwxNPPIHly5dj9erVyM3NxUsvvWTUJiQkBAkJCUbFRFFREeLj46t03BMnToSIYMSIESY3+xUWFmL48OEQEUycOLFS61SUnZ0dfHx8SjzXl3e/Mkf37t2h0+lM+ty+fTtEBMHBwUbLLXJerrK7D8ygerTwXvfe7Ge4gdDLy0u6desmW7Zskbi4OGncuLHUqFHD6BGLgwcPCgCJioqSnTt3yurVq6Vly5bi7e1t1KeIiLe3tzRt2lR27twpycnJZd4g4+PjI3379jVZnpmZKfHx8RIfHy/R0dECQBYtWiTx8fFGj4tcvHhRnJycpH79+vLZZ59JQkKCLFu2THx8fKRatWpGz18bHn8s63FHkT9vpDRs/7nnnit+fDE+Pt7o5pkjR46InZ2d+Pn5yYoVKyQhIUHmz58v9erVE1dXV5NHgo4ePSoAZPPmzWWO4VFT2mNQpd0gd++++uWXXwoAGT58uOzdu1diYmLE19dXmjRpYnQD561bt+Txxx8Xf39/uXXrloiInD17VmrVqiXdunUrc5ylHSP3+usEQv369ZNNmzbJvn37ZP78+eLl5VXmpEPmrjNo0CCxt7cv3r/++q+sR2NLO37S0tLEyclJmjZtKmvWrJGtW7dK9+7dRafTmTyGa7jpuCx5eXnSunVrqVWrlsybN092794tEyZMEGtra3nttddM2s+ePVusra1NHiV7mEVHR4ter5c5c+bI3r175eOPP5a6deuKp6dniTdRL1myRACIp6enPPnkkyZ5amqq2NvbS4sWLWTdunWyefNmef7558Xb21sAyIULF5TjKe2YKslfJxCKjY2V/fv3S2xsrLRr106srKxk/vz5lV7H8DNn5syZJe6vJd2YZzBkyBBxd3c3WV6e/crw/UhMTCzz+/Haa6+JlZWVjB07Vnbv3i0LFy4UZ2dnadWqleTm5hq1tcR5+ZErBlatWiWvv/661K1bV/R6vXTo0EGOHDlisv6KFSvEz89P7O3txd/fX9atW2fSp4jInj17pFWrVqLX6836wfvOO++Is7OzyZ33hvGV9O/eg/LMmTMycOBAadiwoej1emnQoIH07t1bTpw4YdTup59+EgDy1ltvKcck8r/vbUn/7n1NKSkp0r17d/H09BS9Xi8+Pj4ybNgwuXjxYomv18XF5aGZNbIqVbYYEBGZMWNG8fvo5+cnS5cuLe7XYMCAAeLo6Gjy/sbHxwsAmTNnjnKc5hYDIn/O8BcXFyehoaHi5OQkdnZ20qhRIxk+fHipJ/LyrjNo0KBS97V7j697lXb8iPy5v4eHh0uNGjXE3t5egoOD5dtvvzVpFxgYKG5ubmZ9P7KysuSVV14RV1dXsbW1lccee0xmzZplNIeJQYcOHaRr165m9fuwyM7OlqFDh0q9evXE0dFR2rdvLwcOHCj1iaobN26Ig4ODAJClS5eW2OeBAwfkiSeeEL1eL25ubvLmm2/KzJkzBYBcv35dOZ7yFAMifz5JEhUVJa6urmJjYyP16tWTHj16yKFDh6pkHdV58d6fRffau3evAJAffvjBJDN3vxo3bpzodDo5depUmd+LgoICmTFjhjRu3FhsbW3F3d1dhg8fXmJxaonz8iMzA6Hhh+29nxLutytXroidnV2ZjxdWhYULF0q1atUkIyPD4tsqSUFBgTRs2FAmTZr0QLZPfz+VPX5u3rwpNjY2smDBgiodV1pamuh0ukpNffx31rlzZ2nSpMmDHsZ917x58xIfLzRXmzZtJCoqqgpHZLnzMouBChg/frw0b968xE8XVSkqKkomTpxo0W2oxMTEiIuLyyN12ZQefpU5frZs2SLe3t4ml00ra/DgwfLMM89UaZ+PqjFjxsjKlSslMTFR1q9fLz169BAA8sUXXzzood1327dvF3t7+1JnPVS5ceOG2NnZmczqWlmWOi8/FPMMPGomT54MR0dHXLlypUoeuSpNVd+0U15FRUWIi4szmYGLqDIqc/yEh4cjPDy8SsdTUFAAX1/fKrnx7O+gsLAQU6ZMQUZGBnQ6Hfz9/bFq1SoMGDDgQQ/tvnvuuecwa9YsnDt3rtw3d9esWbPUJ4sqw1LnZZ2ImbM5EBER0d/SI/doIREREVUtFgNEREQax2KAiIhI41gMEBERaZzZTxP8+nKMMt8zeb0yd7ZboMy3zFZPd7p/1AVlDgA/PhatzIP/q75XcvDIF5X5HyljlPn0V1or8zOjnlLmNwrKuLP6bH9lPPaDsv+Wdpc5zZX5lmO/KPMXWlRT5ls/bKDMt325Wplbwq9FZe87lVG3YJVF+weAvLyhFu3/RmHJfxugqjhePVR2o0q65tXEov03sivfX6KrCh9/MaTsRpVgdfZXi/YPAJk3q/bpj3vZt7Etu1Fl+v9ln0X7B4CUxxpZtP+v+39YZhteGSAiItI4FgNEREQax2KAiIhI41gMEBERaRyLASIiIo1jMUBERKRxLAaIiIg0zux5Bv41a4QyHzdOPc/A+shJynzo6XxlPjarizIHgKSIb5V58NcblXnzNf2U+aoX1H+B6rEdDsp8Q9Dbyjxxq3oego1DZinzrJablDkApOzJUuYTpn6gzKt/c0OZX1rctswxEBHRw4VXBoiIiDSOxQAREZHGsRggIiLSOBYDREREGsdigIiISONYDBAREWkciwEiIiKNM3uegUvdXNQd/fSlMl+y9KAy/2rJQGXe3H+vMgeAZ3+5qMw/7l6kzMPcuyrz4xKkzMfGBSjz5V+8qsyXLJiuzMPbnVTmM4K/UOYAsHPdm8r8xHxR5lGtXlHm+tHvqgewaKw6JyKi+45XBoiIiDSOxQAREZHGsRggIiLSOBYDREREGsdigIiISONYDBAREWkciwEiIiKNM3uegczofynzDtm/KHO7jDHKfNTxxcq87zPPKHMAuJA7RJnnB32qzIOyd6rXvzFKmb/rFajMf+wySZlH/L5PmQ+b+09lfn1R2fMMTH6ulzKf7f6OMo/KO6/Mi15W5w9CjQz13AmVdSdJvW9XBcdu9hbt3+lknkX7v+rd0aL9A8DdnNuW3YCdZbsvyZkrj1m0//DzdS3aPwDU8T9v0f6Tz9W3aP8ODi9YtH8AaHY70+LbKAuvDBAREWkciwEiIiKNYzFARESkcSwGiIiINI7FABERkcaxGCAiItI4FgNEREQaZ/Y8Ax3+r0iZf1ctVplv/ai1Mj/3wXBlXv1X9TwGALBnb09lHjLwQ2W+osMqZT7r06nKvMaGJGXewqmfMv8j/g1lHvhGO2V+/tosZQ4AvqN8lPl2l5HKfKbHJWV+bEIX9QB2PfjnaYmIyBivDBAREWkciwEiIiKNYzFARESkcSwGiIiINI7FABERkcaxGCAiItI4FgNEREQax2KAiIhI48yedCjH9zllvmxtiDJvGOymzHcvaanM64y+oswB4NbOJ5X5gWYe6g7uPquMD/Vqrsx/nZyuzBf5dVPmz427o8x3LjqgzFs0OKXMAaDZx9WU+bfPva3MZ//jsjL3aDi6zDEQEdHDhVcGiIiINI7FABERkcaxGCAiItI4FgNEREQax2KAiIhI41gMEBERaRyLASIiIo0ze56Boa+on/Mv3POrMh/99SfKPCXxuDJPdiy7bjnWaL0yH/drmDLvGherzBv+x0GZ7zkbrcxPzL6lzBcMn6XMJ733uTJ3fr7st3PM1Rhl/prHf5V5jej2yrzNhjlljGBSGXnVK3C6a9H+Lz9h+Zq62a1LFu3/qr+XRfs/e/OQRfsHAN2ddpbdQE3Ldl+SpzNuWrT/A8+3sWj/AOCZlmbR/ms9dsOi/aefO23R/gHA8VCRZTfwz7Kb8MoAERGRxrEYICIi0jgWA0RERBrHYoCIiEjjWAwQERFpHIsBIiIijWMxQEREpHFmzzPgdbK7Mr/4Xaoy/2TKq8r8fPBRZb6zxjxlDgARU79X5q9vWazMt305W5nXbp2gzN8uUs8jkOw2QZl/2ddOmdv/qH5e3mnju8ocANyKvlTmp93VzzV3nqKeL+LOri1ljoGIiB4uvDJARESkcSwGiIiINI7FABERkcaxGCAiItI4FgNEREQax2KAiIhI41gMEBERaZzZ8wwsWTlAmQ91yVLmgRfmK/OCg22V+VbbqcocAOacWaHMzzf9Q5lP+ekpZd5h9iBl7uLZVZnfOqX+o9Kv68Yp865dC5V50lfnlDkAhLfNU+Y9219R5s8dV/fvMW2IMu+jXp2IiB4AXhkgIiLSOBYDREREGsdigIiISONYDBAREWkciwEiIiKNYzFARESkcSwGiIiINM7seQbO7uupzL94cpEyD/Afpcyf+j5Omff8wUOZA8AXy75U5v9s+7UyPzlluTKfMqaRMj/rPkaZO6dvUOa9T6rnanj/qUPKPO/fucocAP7fXfVcBNGr1yjzIfuPKvOICanqAQxWx5aQfLCWRft/puMei/YPAJkFYRbtP/s39fwTlRWg97Vo/wBwKUssuwFPy3Zfkh3Wthbt/7Frlt93sy77W7T/fOuzFu0/4Dc/i/YPAHkOThbfRll4ZYCIiEjjWAwQERFpHIsBIiIijWMxQEREpHEsBoiIiDSOxQAREZHGsRggIiLSOLPnGRjldUqZ97ylfl61e5MUZe7r/ocy77+3nTIHgJF37ZT59sADylwWqV+jp09HZT4oKE2Z/5QSrcznvVRXmW/Y8ZUyrzlevX0A+KqVjzIf/576Nb6Yt0yZn3jBVZl3UaZERPQg8MoAERGRxrEYICIi0jgWA0RERBrHYoCIiEjjWAwQERFpHIsBIiIijWMxQEREpHFmzzMQe6y3MvefNU2ZS8YWZd7ttvoZ/9uzFipzADgxeLcy39VvujJf9b76Of8+1luV+ZVnf1PmJz9R/033j8ccVOYOex2UeeDbDZU5AExYMk2ZL7dRvw/jDi9S5k/PnVDmGIiI6OHCKwNEREQax2KAiIhI41gMEBERaRyLASIiIo1jMUBERKRxLAaIiIg0jsUAERGRxpk9z0C090llXnhonzLffGqTMp+X1FOZjxv/tjIHgON9Dyvzob32KvNjoS8oc4dJnZT5tA/V385XV99V5kFn31Xmzrd0yvz5+bnKHAA+GhCuzIc/21mZR4UMVuabpv1U5hjut394qOd/qKzTVyMs2j8AVLt1yaL933TdYdH+Ha1DLNo/APifMvt0VjEB9SzbfwlC7Cy7zevpLhbtHwB+dbLsOcHB9w+L9p9ZI82i/QPAnZT6Ft9GWXhlgIiISONYDBAREWkciwEiIiKNYzFARESkcSwGiIiINI7FABERkcaxGCAiItI4sx/MbbBklzL3naR+Pv3XV95T5ksONlHmqQPLftYzdWGGMk/prn5O/92hA5R5sOsUZf722nXKfFbGfmW+9Hn1XAr/aNhCme/uckuZA8D+ZPVb/slH85X55V/V75PTB2U8c/8vdUxERPcfrwwQERFpHIsBIiIijWMxQEREpHEsBoiIiDSOxQAREZHGsRggIiLSOBYDREREGsdigIiISOPMnnRoztvqSYXqttmnzNf+tFyZtxowXZlnvvG9MgcA3UIXZf7zs9uV+a2UVcq83dxUZV6vd0dlXniyoTL/fbSPMh8xd4Qyn/LqSWUOAO1X9VLmv339uTLf2T1cmS+5nlXmGIiI6OHCKwNEREQax2KAiIhI41gMEBERaRyLASIiIo1jMUBERKRxLAaIiIg0jsUAERGRxpk9z8D7H15V5jMWfKTMf35stDJP985X5l33hylzAHjrwhllnjFzrzJv6fWYMv8y55oy/zXvP8q8aP1Xyjzs/XHKfNvTzZR5wu4xyhwA2obmKPONnRcpc1en/so8tX9kGSPwKyOvetm6PIv2/0e1bIv2DwA2+bYW7d+z+vMW7b/Of9wt2j8A5OQmWLT/6lDPtWIJN/WpFu3/hEdri/YPANkXLXvMd/zujkX7r21/16L9A4BD7ZoW30ZZeGWAiIhI41gMEBERaRyLASIiIo1jMUBERKRxLAaIiIg0jsUAERGRxrEYICIi0jiz5xk4eH2gMveMTFLm0a9fVOYRyW8q8w+unlbmAFBr/VhlnnB7qjJ/Pv5TZT7s6Dplfrx3ljIf1WO5Mg/5l3oegwXft1Lm7UetUeYA0PDt35V5uwInZV7reB9l/k63ModAREQPGV4ZICIi0jgWA0RERBrHYoCIiEjjWAwQERFpHIsBIiIijWMxQEREpHEsBoiIiDTO7HkGcnvplPnR9/9PmU+9+70yzwq8osxrxbdX5gDQYVdjZT5IP12ZD3NS10aTWqnnEfjE8bAyzz/9nTKvnf+GMg+5cVKZfxbjo8wBoEWw+m+yr255SJlXu6b+Hjyb/eD/LjcREZUPrwwQERFpHIsBIiIijWMxQEREpHEsBoiIiDSOxQAREZHGsRggIiLSOBYDREREGqcTEXnQgyAiIqIHh1cGiIiINI7FABERkcaxGCAiItI4FgNEREQax2KAiIhI41gMEBERaRyLASIiIo1jMUBERKRxLAaIiIg07v8D6TQBDFpHURAAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# testing\n",
    "A_prev, max_params, avg_params = t7.pool_fwd_test()\n",
    "A_maxpool, cache = pool_fwd(A_prev, max_params)\n",
    "A_avgpool, _ = pool_fwd(A_prev, avg_params)\n",
    "print(\"mean(A_maxpool) = {:.3f}\".format(np.mean(A_maxpool)))\n",
    "print(\"mean(A_avgpool) = {:.3f}\".format(np.mean(A_avgpool)))\n",
    "print(\"{}x{} CONV with stride {}\".format(cache['n_f'], cache['n_f'], cache['stride']))\n",
    "\n",
    "_, figs = plt.subplots(1, 3)\n",
    "figs[0].set_title('input {}'.format(A_prev.shape[1:3]))\n",
    "figs[1].set_title('max POOL {}'.format(A_maxpool.shape[1:3]))\n",
    "figs[2].set_title('avg POOL {}'.format(A_avgpool.shape[1:3]))\n",
    "figs[0].imshow(A_prev[0,:,:,:])\n",
    "figs[1].imshow(A_maxpool[0,:,:,:])\n",
    "figs[2].imshow(A_avgpool[0,:,:,:])\n",
    "_ = [fig.axis('off') for _, fig in np.ndenumerate(figs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> `mean(A_maxpool) = 0.900\n",
    "mean(A_avgpool) = 0.501\n",
    "3x3 CONV with stride 3`\n",
    "\n",
    "\n",
    "## D. Pool Layer Backprop ##\n",
    "\n",
    "Let's now implement the backward pass for the POOL layer for both max and average pooling modes. Even though a pooling layer has no parameters to update, we still need to backprop the gradient through the pooling layer in order to compute gradients for previous layers (typically a CONV layer).\n",
    "\n",
    "### D.1. Pooling mask  ###\n",
    "\n",
    "Let's first consider the max pooling mode. We must consider the location of the maximum values in the input maps, creating a mask slice from the pooling region. Note that it is possible to have multiple entries of the max value. In this case, we will scale the mask by the number of max values. This mask indicates input locations that affect the pooling output. Similarly we can define a pooling mask for the average pooling mode. In this case, all the values in the slice window affect the pooling output equally. So the mask will be a matrix of ones scaled by the average pooling number of elements.\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 8:**</font> Implement the function to create the pool mask for a given slice. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_mask(a_slice, pool_mode):\n",
    "    \"\"\"\n",
    "    Creates a mask from input sclice\n",
    "    \n",
    "    Arguments:\n",
    "    a_slice -- Array of shape (n_f, n_f)\n",
    "    pool_mode -- String either 'max' or 'avg'\n",
    "    \n",
    "    Returns:\n",
    "    mask -- Array of shape (n_f, n_f)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### INPUT YOUR CODE HERE ### (5 lines)\n",
    "    if pool_mode == 'max':\n",
    "        mask = (a_slice == np.max(a_slice))\n",
    "    elif pool_mode == 'avg': \n",
    "        mask = np.ones_like(a_slice) / a_slice.size\n",
    "    mask = mask.astype(int)\n",
    "    ### END OF YOUR CODE SEGMENT ###\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_slice:\n",
      "[[9. 4. 6.]\n",
      " [6. 9. 3.]]\n",
      "max mask:\n",
      "[[1 0 0]\n",
      " [0 1 0]]\n",
      "avg mask:\n",
      "[[0 0 0]\n",
      " [0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "np.random.seed(2019) # for reproducibility\n",
    "a_slice = np.round(10 * np.random.rand(2, 3))\n",
    "print('a_slice:')\n",
    "print('{}'.format(a_slice))\n",
    "mask = slice_mask(a_slice, 'max')\n",
    "print('max mask:')\n",
    "print('{}'.format(mask))\n",
    "mask = slice_mask(a_slice, 'avg')\n",
    "print('avg mask:')\n",
    "print('{}'.format(mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> `a_slice:\n",
    "[[9. 4. 6.]\n",
    " [6. 9. 3.]]\n",
    "max mask:\n",
    "[[0.5 0.  0. ]\n",
    " [0.  0.5 0. ]]\n",
    "avg mask:\n",
    "[[0.167 0.167 0.167]\n",
    " [0.167 0.167 0.167]]`\n",
    "\n",
    "\n",
    "### D.2. POOL Layer Backprop ###\n",
    "  \n",
    "Let's now compute the backward propagation of the POOL layer. \n",
    "\n",
    "> <font color='darkgreen'>**Exercise 9:**</font> Implement the function below using four nested loops to compute the POOL layer backprop using the `slice_mask` function above to compute the mask to be mutliplied by the $dZ_{i,h,w,c}$ scalar using broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pooling layer backward propagation\n",
    "def pool_back(dA, cache):\n",
    "    \"\"\"\n",
    "    Implements pooling backward propagation \n",
    "    \n",
    "    Arguments:\n",
    "    dA -- gradient of loss with respect to pooling layer output, same shape as A\n",
    "    cache -- dictionary for backprop\n",
    "        A_prev -- pool layer input\n",
    "        stride -- integer\n",
    "        n_f -- integer\n",
    "        pool_mode -- string either 'max' or 'avg'\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of loss with respect to pooling layer input, same shape as A_prev\n",
    "    \"\"\"\n",
    "    \n",
    "    ### INPUT YOUR CODE HERE ### (7 lines)\n",
    "    A_prev = cache['A_prev'] \n",
    "    stride = cache['stride'] \n",
    "    n_f = cache['n_f']\n",
    "    pool_mode = cache['pool_mode']\n",
    "    n, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "    _, n_H, n_W, n_C = dA.shape\n",
    "    dA_prev = np.zeros_like(A_prev)\n",
    "    ### END OF YOUR CODE SEGMENT ###\n",
    "\n",
    "    \n",
    "    for i in range(n):                    # batch loop\n",
    "        a_prev = A_prev[i, :, :, :]       # select training example from A_prev\n",
    "        for h in range(n_H):              # height loop\n",
    "            for w in range(n_W):          # width loop\n",
    "                for c in range(n_C):      # channel loop\n",
    "                    ### INPUT YOUR CODE HERE ### (5 lines)\n",
    "                    h_range = slice(h*stride, h*stride+n_f)\n",
    "                    w_range = slice(w*stride, w*stride+n_f)\n",
    "                    a_prev_slice = A_prev[i, h_range, w_range, c]\n",
    "                    mask = slice_mask(a_prev_slice,pool_mode)\n",
    "                    dA_prev[i, h_range , w_range, c] += mask * dA[i, h, w, c]\n",
    "                    ### END OF YOUR CODE SEGMENT ###\n",
    "\n",
    "    assert(dA_prev.shape == A_prev.shape)    \n",
    "    return dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max-pool: mean(dA_prev) = 0.055, std(dA_prev) = 0.184\n",
      "avg-pool: mean(dA_prev) = 0.000, std(dA_prev) = 0.000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# testing\n",
    "A_prev, max_params, avg_params = t7.pool_fwd_test()\n",
    "\n",
    "A, cache = pool_fwd(A_prev, max_params)\n",
    "dA = np.random.rand(*A.shape)\n",
    "dA_prev = pool_back(dA, cache)\n",
    "print(\"{}-pool: mean(dA_prev) = {:.3f}, std(dA_prev) = {:.3f}\".format(cache['pool_mode'], np.mean(dA_prev), np.std(dA_prev)))\n",
    "\n",
    "_, figs = plt.subplots(1, 4)\n",
    "figs[0].imshow(A_prev[0,:,:,:])\n",
    "figs[1].imshow(dA_prev[0,:,:,:])\n",
    "figs[2].imshow(A[0,:,:,:])\n",
    "figs[3].imshow(dA[0,:,:,:])\n",
    "figs[0].set_title('$A^{k-1}$')\n",
    "figs[1].set_title('$dA^{k-1}$')\n",
    "figs[2].set_title('$A^{k}$')\n",
    "figs[3].set_title('$dA^{k}$')\n",
    "_ = [fig.axis('off') for _, fig in np.ndenumerate(figs)]\n",
    "\n",
    "A, cache = pool_fwd(A_prev, avg_params)\n",
    "dA_prev = pool_back(dA, cache)\n",
    "print(\"{}-pool: mean(dA_prev) = {:.3f}, std(dA_prev) = {:.3f}\".format(cache['pool_mode'], np.mean(dA_prev), np.std(dA_prev)))\n",
    "\n",
    "_, figs = plt.subplots(1, 4)\n",
    "figs[0].imshow(A_prev[0,:,:,:])\n",
    "figs[1].imshow(dA_prev[0,:,:,:])\n",
    "figs[2].imshow(A[0,:,:,:])\n",
    "figs[3].imshow(dA[0,:,:,:])\n",
    "figs[0].set_title('$A^{k-1}$')\n",
    "figs[1].set_title('$dA^{k-1}$')\n",
    "figs[2].set_title('$A^{k}$')\n",
    "figs[3].set_title('$dA^{k}$')\n",
    "_ = [fig.axis('off') for _, fig in np.ndenumerate(figs)]\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> `max-pool: mean(dA_prev) = 0.055, std(dA_prev) = 0.184\n",
    "avg-pool: mean(dA_prev) = 0.055, std(dA_prev) = 0.032`\n",
    "\n",
    "## Final Remark ##\n",
    "\n",
    "You now ready to create deep convolutional models stacking CONV and POOL layers. Note that our implementation is not fully vectorised (4 nested loops in forward and barckward propagation) and as a result training large models will be slow. Also we don't make use of massively parallel MAC achitectures as found in GPU. For this reason, we won't train any model with this code and will prefer to use deep learning frameworks such as tensorflow.keras or pytorch.nn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- EOF --"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "wRuwL",
   "launcher_item_id": "NI888"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
